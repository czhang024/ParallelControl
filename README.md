<h1 align="center">
    <p> From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on
LoRA with Parallel Control <br> 
    [ICML 2025 (Spotlight)] ğŸŒŸ</p>
</h1>

<div align="center">
  <img src="https://img.shields.io/badge/ICML-2025-red?style=for-the-badge&logo=arxiv" alt="ICML 2025">
  <img src="https://img.shields.io/badge/Spotlight-2.59%25-gold?style=for-the-badge" alt="Acceptance Rate">
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white" alt="PyTorch">
  <img src="https://img.shields.io/badge/Memory-Efficient-green?style=for-the-badge" alt="Memory Efficient">
</div>

<div align="center">
  <h3>ğŸ”¥ The Official PyTorch Implementation ğŸ”¥</h3>
  <p><strong>ğŸ“„ Paper:</strong> <a href="TBD">From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control</a></p>
  <p><strong>âœ¨ Spotlight Paper at ICML 2025 (Acceptance Rate: 2.59%)</strong></p>
</div>

---

## ğŸ¯ What's New?

ğŸ’¡ **New Understandings to PEFT**: Transform traditional weight-based fine-tuning into state-based optimization!

ğŸ’¡ **Memory Reductions**: Further reduce the memory costs for large models!

---

## ğŸ“ Repository Structure

### 1. [`RoBERTa`](./RoBERTa/)
- Experiments using **RoBERTa** on the **GLUE benchmark**
- Performance comparisons: Control vs LoRA vs DoRA
- Instructions for replicating results on all 8 GLUE tasks


### 2. [`ViT`](./ViT/) 
- **Vision Transformer (ViT)** experiments on image classification
- Support for multiple vision datasets
- Memory usage comparison analysis


### 3. [`LlaMA`](./LlaMA/) 
- Experiments with **LLaMA2** and **LLaMA3** models
- Commonsense reasoning task evaluations
- Fine-tuning scripts for large-scale models


### 4. [`QControl`](./QControl/) - Quantization Integration
- **Quantization** support for all methods (Control/LoRA/DoRA)
- Optimized for **RoBERTa** models
- Ultra-low memory footprint training



---

## ğŸš€ Quick Start Guide

### ğŸ“‹ Prerequisites
```bash
# ğŸ Python 3.9+
# ğŸ”¥ PyTorch 1.12+
# ğŸ¤— Transformers
# ğŸ“Š Additional dependencies in each subdirectory
```

### ğŸ› ï¸ Installation
```bash
# Clone the repository
git clone https://github.com/czhang024/ParallelControl

# Install custom peft
pip install -e peft

# ğŸ“‚ Navigate to your desired experiment folder
cd RoBERTa  # or ViT, LlaMA, QControl
```

---

<!-- ## ğŸ“š Citation

If you use this work in your research, please cite our paper:

```bibtex
@inproceedings{your2025memory,
  title={From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on LoRA with Parallel Control},
  author={Your Name and Co-authors},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025},
  note={Spotlight Paper, Acceptance Rate: 2.59\%}
}
``` -->

---

## ğŸ™ Acknowledgments

Special thanks to:
- ğŸ›ï¸ The **ICML 2025** review committee
- ğŸ¤— **Hugging Face** for the Transformers library
- ğŸ”¥ **PyTorch** team for the framework

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">
  <h3>ğŸŒŸ Star this repo if you find it helpful! ğŸŒŸ</h3>
</div>
