<h1 align="center">
    <p> From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on
LoRA with Parallel Control <br> [ICML 2025 (Spotlight)]</p>
</h1>

The Official PyTorch implementation of [**From Weight-Based to State-Based Fine-Tuning: Further Memory Reduction on
LoRA with Parallel Control**](TBD) [ICML 2025 (Spotlight, acceptance rate: ***2.59%***)].

## üìÅ Directory Structure

### 1. [`RoBERTa_GLUE`](./RoBERTa_GLUE/)
Experiments using **RoBERTa** on the **GLUE benchmark** for natural language understanding.  
Includes training, evaluation scripts, and instructions for replicating results on all GLUE tasks.

### 2. [`ViT`](./ViT/)
Experiments with **Vision Transformer (ViT)** on image classification tasks.  
Contains model training code, data preprocessing scripts, and usage guidelines.

### 3. [`LlaMA`](./LlaMA/)
Commonsense reasoning tasks using **LLaMA2** and **LLaMA3** models.  
Includes scripts for prompt-based evaluation, fine-tuning, and detailed usage instructions.

### 4. [`QControl`](./QControl/)
Include Quantization for Control/LoRA/DoRA on the RoBERTa model.

---

## üì¶ How to Use

Each subdirectory contains:

- Instructions for installation and usage (`README.md` in each folder)
- Model-specific code
- Scripts for training and evaluation

Please refer to the individual folders for detailed setup and execution guidance.

---